package com.vishal.aiyoutube.ai_analysis_service.config;

import com.fasterxml.jackson.databind.DeserializationFeature;
import com.fasterxml.jackson.databind.ObjectMapper;
import com.vishal.aiyoutube.ai_analysis_service.dto.GrokChatResponse;
import lombok.RequiredArgsConstructor;
import lombok.extern.slf4j.Slf4j;
import org.springframework.beans.factory.annotation.Value;
import org.springframework.stereotype.Component;
import org.springframework.web.reactive.function.client.WebClient;
import java.util.ArrayList;
import java.util.HashMap;
import java.util.List;
import java.util.Map;

/**
 * GrokClient serves as the primary gateway for interacting with the Groq/Grok AI models.
 * It handles the construction of OpenAI-compatible requests and processes the AI-generated
 * synthesis for the YouTube intelligence pipeline.
 */
@Slf4j
@Component
@RequiredArgsConstructor
public class GrokClient {

    /**
     * Non-blocking WebClient pre-configured for high-concurrency AI requests.
     */
    private final WebClient grokWebClient;

    /** * Secure API key retrieved from application properties (grok.api-key).
     */
    @Value("${grok.api-key}")
    private String apiKey;

    /** * Specific AI model to be used (e.g., llama-3.3-70b-versatile or grok-1).
     */
    @Value("${grok.model}")
    private String model;

    /** * Controls the randomness of the AI output. 0.0 is deterministic, 1.0 is creative.
     */
    @Value("${grok.temperature}")
    private Double temperature;

    /**
     * Specialized ObjectMapper configured to be 'lenient'.
     * It will ignore any new or unknown fields returned by the AI provider to prevent
     * the application from crashing during upstream API updates.
     */
    private final ObjectMapper lenientMapper = new ObjectMapper()
            .configure(DeserializationFeature.FAIL_ON_UNKNOWN_PROPERTIES, false);

    /**
     * Orchestrates a Chat Completion request to the AI model.
     * * @param systemPrompt Defines the AI's persona and rules (e.g., "Act as a financial analyst").
     * @param userPrompt The actual transcript data or query to be processed.
     * @return The text-based content generated by the AI.
     */
    public String chat(String systemPrompt, String userPrompt) {
        // Prepare the request payload as a Map for easy JSON serialization
        Map<String, Object> requestBody = new HashMap<>();
        requestBody.put("model", model);
        requestBody.put("temperature", temperature);

        // Build the message list (System roles guide the AI, User roles provide the data)
        List<Map<String, String>> messages = new ArrayList<>();
        messages.add(Map.of("role", "system", "content", systemPrompt));
        messages.add(Map.of("role", "user", "content", userPrompt));
        requestBody.put("messages", messages);

        log.info("Dispatching AI analysis request using model: {}", model);

        /**
         * Execute the synchronous POST request.
         * Note: While WebClient is reactive, we use .block() here because the
         * final report synthesis is a terminal step in our business logic chain.
         */
        String rawResponse = grokWebClient.post()
                .uri("/openai/v1/chat/completions")
                .header("Authorization", "Bearer " + apiKey)
                .header("Content-Type", "application/json")
                .bodyValue(requestBody)
                .retrieve()
                /**
                 * Error Handling Strategy:
                 * Maps 4xx/5xx status codes into RuntimeExceptions, exposing the
                 * raw error body from the AI provider for faster debugging.
                 */
                .onStatus(status -> status.isError(), response ->
                        response.bodyToMono(String.class)
                                .map(body -> new RuntimeException("Groq API Error: " + body))
                )
                .bodyToMono(String.class)
                .block();

        try {
            // Parse the raw JSON string into our structured DTO
            GrokChatResponse response = lenientMapper.readValue(rawResponse, GrokChatResponse.class);

            /**
             * Observability:
             * Logs the total token consumption (Prompt + Completion).
             * Vital for monitoring costs and context window limits in production.
             */
            if (response.getUsage() != null) {
                log.info("Analysis Service Usage -> Total Tokens: {}", response.getUsage().getTotal_tokens());
            }

            // Extract and return the actual text from the first choice in the response
            return response.getChoices().get(0).getMessage().getContent();
        } catch (Exception e) {
            log.error("Failed to process AI response payload for Topic.");
            throw new RuntimeException("Failed to parse analysis response", e);
        }
    }
}